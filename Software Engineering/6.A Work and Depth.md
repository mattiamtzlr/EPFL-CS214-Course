#software-engineering #parallelism 

Talking about asymptotic complexity of [[6.1 Parallelism|parallel code]] depends on available parallel resources (e.g. number of CPU cores).
For this, we introduce two *measures* for a program.

# Work and Depth
#terminology 

**Work** `W(e)`: number of steps `e` would take if there was no parallelism:
- This is just the sequential execution time,
- thus we can treat all `parallel(e1, e2)` as `(e1, e2)`, because all work still needs to be done.

**Depth** `D(e)`: number of steps if we had unbounded parallelism (e.g. infinitely many CPU cores):
- Take the maximum of running times for arguments of `parallel`.
- If the task can be split into two, it may be done twice as soon.

The **key insight** here is, that if the depth is too large, no amount of parallel processing will make the code fast.

## Rules
The key rules are:
- `W(parallel(e1, e2)) = W(e1) + W(e2) + c2`
- `D(parallel(e1, e2)) = max(D(e1), D(e2)) + c1`

where `c1`, `c2` are constants.
If we divide work in equal parts, for depth it counts only once.

For code where we don't use `parallel` explicitly, we must *add up the costs*. For instance, for a function call or operation `f(e1, ..., eN)`:
- `W(f(e1, ..., eN)) = W(e1) + ... + W(eN) + W(f)(v1, ..., vN)`
- `D(f(e1, ..., eN)) = D(e1) + ... + D(eN) + D(f)(v1, ..., vN)`

where `vI` denotes values of `eI`. If `f` is a primitive operation on integers, then `W(f)` and `D(f)` are constant, regardless of `vI`.
