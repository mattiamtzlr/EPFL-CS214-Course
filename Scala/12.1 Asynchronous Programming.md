#scala #parallelism 

Asynchronous computing commonly follows the pattern:
- launch a task that *takes some time*
- some time later, we need the result of the task
- if there are no side-effects, the task can be executed *concurrently* with the rest of the program.

This is beneficial, as we can use [[6.1 Parallelism|parallel computing resources]] to save time.

However, the classical *fork/join*-construct form parallel programming is close but too restrictive.

# The `Future` Construct
This topic will be split up into three parts, *simple* / *direct-style* futures, *asynchronous execution* and *completable* futures

## 1. Simple Futures
Consider a class `SimpleFuture`, which splits the points in time, where a task is *defined* and where its result is *needed*:
```Scala
// definition
val f = SimpleFuture{ <some task> }

// result needed
f.await
```
Note: this class doesn't really exist in Scala, this is just for illustrative purposes.

We could then use the class as follows:
```Scala
@main def main = {
  def compute(x: Double): SimpleFuture[Double] =
    SimpleFuture:
      Thread.sleep(1000) // simulate long running task
      x * x

  val fs = (1 to 1000).map(compute(_)) // definition
  val xs = fs.map(_.await)             // result needed

  println(xs.sum)
}
```

This process defines a new evaluation strategy:

### Lenient Evaluation
#terminology 

Up until now, we have seen two evaluation strategies, *strict* evaluation and *lazy* evaluation.

*Lenient* evaluation is a third option, kind of between the two:
> An expression can be evaluated *as soon as* it is defined, and *must* be evaluated once its value is *needed*.

This is great for exploiting parallelism and is related to *dataflow programming* i.e. evaluating when the data is ready.

### Implementation
A simple implementation of `SimpleFuture` could be the following:
```Scala
class SimpleFuture[T](body: => T):
  private var status: Option[Try[T]] = None

  private val thread = new Thread:
    override def run(): Unit =
      status = Some(Try(body))
    start()

  def awaitTry: Try[T] =                 // Futures always return
    if status.isEmpty then thread.join() // either Success or Failure
    status.get

  def await: T = awaitTry.get // Short form: throw exception on Failure
end SimpleFuture
```

However, threads are a scarce and costly resource, we can't have too many of them running at the same time.
A better idea would be to have a *thread pool* of fixed length and a *task queue* which gets executed by the thread pool.

## 2. Asynchronous Execution
The same problems can be observed in *asynchronous execution*, that is:
- Execution of a computation on *another computing unit*, without waiting for its termination.
- This gives better resource efficiency.

**Example: Coffee shop**
Consider a coffee shop, where we need fewer sellers than makers, as the seller's job takes less time than the maker's. We can then use multiple makers per seller to maximise throughput.
![[coffee_shop_asynchronous.png|500]]

### Inversion of Control
#terminology 

This is called inversion of control - *don't call us, we'll call you*.
Instead of running thousands of threads, each for one task, we prepare the tasks and run them from a fixed amount of scheduler threads (1+).

Every task has to run to completion without blocking, thus if another task needs to run first before, that task has to be prepared as a work item and needs a *callback* which calls the original task upon completion.

However, callbacks quickly get very nested and long, if there are many tasks depending on each other, take for example a booking portal (abstracted):
```Scala
queryWhere: callback1 =>
  queryWhen: callback2 =>
    findHotels: callback3 =>
      getRates: callBack4 =>
        getUserChoice: callBack5 =>
          getCreditCardInfo: callBack6 =>
            getUserConformation: callBack7 =>
              getHotelConfirmation: callBack8 => ...
```
Also, parallel queries are hard to implement and error handling gets complicated as well.

## 3. Completable Futures
The `scala.concurrent.Future` trait is defined similarly to the following:
```Scala
trait Future[+T]:
  def onComplete(f: Try[T] => Unit)(using ExecutionContext): Unit

  // transformations of successful results
  def map[S](f: T => S): Future[S]
  def flatMap[S](f: T => Future[S]): Future[S]
  def zip[U](that: Future[U]): Future[(T, U)]

  // transform failures
  def recover[U >: T](f: Exception => U): Future[U]
  def recoverWith[U >: T](f: Exception => Future[U]): Future[U]
```
Remark:
- The `map` function automatically propagates the failures (if any) of `Future[A]` to the resulting `Future[B]`.
  <br>
- The `flatMap` function returns a failed `Future[B]` if the former `Future[A]` failed or if the resulting `Future[B]` failed.
  <br>
- The `zip` function returns a failure if any of the two `Future` values failed. It doesn't create any dependency between the two `Future` values.

### Schedulers
Note that we don't have to use any explicit schedulers when using the `onComplete` method of `scala.concurrent.Future`, as we can import the *implicit standard scheduler* with:
```Scala
import scala.concurrent.ExecutionContext.Implicits.global
```

### The Universal `await` Construct
Using *suspensions*, a topic which is discussed in detail on [[scala-week12.pdf#page=82|slides 82 - 99]], we can implement a lightweight and universal `await` construct, callable anywhere, which allows for the expression of simple, direct-style futures:
```Scala
val sum = Future[Int]:
  // c1, c2 are some input streams (file, network, etc.)
  val f1: Future[Int] = Future(c1.read)
  val f2: Future[Int] = Future(c2.read)

  f1.await + f2.await
```
This introduces *structured concurrency*, meaning the the local futures `f1` and `f2` complete before `sum` completes, which could lead to one of them being cancelled because the other failed.