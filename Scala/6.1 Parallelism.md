#scala #parallelism #terminology 

# Background and Theory
> [!TLDR]- TLDR
> Sequential computation: split tasks into multiple steps
> Parallelism: split the task into *independent* tasks (*fork*), solve them in parallel and then combine them again (*join*).

Parallelism is the concept of multiple computations happening *at once* (*simultaneously*) in physically different (parts of) devices.

Examples:
- connected computers (clusters, clouds)
- multiple cores in one chip (mainly CPU) or hundreds of threads on a GPU.
- FPGA's
- vector instructions with up to 512 bits

## Parallel vs Sequential Programming

Difficulties of parallel programming:
- Subsumes (includes, absorbs) sequential programming
- Specifying which parts of computation are or can be made independent.
- Different hardware (parts) need to communicate, thus its efficiency also depends on hardware details.

We need parallelism, as we can no longer really increase CPU clock frequency, as they would generate way too much heat.
With parallelism, we can use multiple cores at once running at a lower frequency, thus consuming *less power* and generating *less heat*. This is also great for mobile devices.

## Scala Threads
Threads in Scala are used to parallelize computations:
```Scala
var result = 0
class MyThread(val k: Int) extends Thread:
	override def run: Unit =
	var i: Int = 0
	while i < 6 do
		println(s"${getName} has counter ${i}")
		i += 1
		Thread.sleep(3)
	result = k

def testThread: Unit =
	val t1 = MyThread(0)
	val t2 = MyThread(1)
	t1.start; t2.start
	Thread.sleep(4)
	t1.join; t2.join
	println(s"result = ${result}")
```
The above code starts two threads, one with `k = 0` and one with `k = 1`. The threads both count up from 0 to 5 and print their current value at each iteration. At the end they set `result` to `k`.

### Race Conditions
*Race conditions* happen when a thread writes to a variable while another thread reads or writes to that variable.
This means that results can *change from run to run*.
Or, worse, the result can even be *ill-defined*, if the read takes long enough, a thread might read a half-old half-new value.

Generally, the "loser" of the race, i.e. the slower thread, decides the final value of the variable.

To avoid race conditions, threads are often synchronized using some sort of locking mechanism, this however can lead to *deadlocks*, where programs freeze.

This is not really a problem in Scala, if the programs are purely functional, thus there are no mutable variables.
For example, to compute
```
f(g(x), h(x))
```
we can compute `g(x)` and `h(x)` in parallel.

# Associativity and Commutativity
As we have seen, parallel computation splits a task or a collection into pieces. Depending on how that split is done, we can obtain *different computation trees*, consisting of the same elements.
To ensure a predictable result, we need want all these trees to give the same result. This is ensured by having *only associative operations* when dealing with parallelism.

*Commutativity*, however, is not necessarily needed for parallelism - except if the operation changes the order of elements.
It is not enough for an operation to be commutative, for it to be paralleliseable. It needs to be associative.
For example, the sum of squares
```
f(x, y) = x² + y²
```
is commutative, but
```
f(f(x, y), z) = (x² + y²)² + z²
f(x, f(y, z)) = x² + (y² + z²)²
```
shows that it is not associative.

## Making an Operation Commutative
Suppose there is a binary operation `g` and a strict total ordering `less`, e.g. a lexicographical ordering of bit representations.
Then the following operation is *commutative*:
```Scala
def f(x: A, y: A) = if less(y, x) then g(y, x) else g(x, y)
```

There is however no such trick for associativity

## Associativity on Tuples
Suppose that `f1: (A1, A1) => A1` and `f2: (A2, A2) => A2` are associative, then 
```
f: ((A1, A2), (A1, A2)) => (A1, A2), 
   ((x1, x2), (y1, y2)) -> (f1(x1, y1), f2(x2, y2))
```
is also associative.