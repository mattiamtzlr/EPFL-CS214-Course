#scala #imperative 

To increase the usefulness of [[10.1 Imperative Programming|imperative programming]], we need to make sure that its use is *safe*, by using constructs such as exceptions, good control flow and caching.

# Exceptions
We can define new exceptions for our programs using a class extension:
```Scala
class ReciprocalOfZero extends Exception

def recip100(v: Int): Int =
  if v == 0 then throw new ReciprocalOfZero
  else 100 / v
```
Note: In addition to the Java-like syntax `throw new Exception`, we can also use `throw Exception()`

## Evaluating Exceptions
When exceptions are used in code, the substitution model needs to be adapted to include a *success* `S` and a *failure* `F` which indicates the thrown exception.
Then, we can evaluate as follows:
```Scala
recip100(1)
  => if 1 == 0 then throw new ReciprocalOfZero else 100 / 1
  => 100 / 1
  => S(100)

recip100(0)
  => if 0 == 0 then throw new ReciprocalOfZero else 100 / 0
  => throw new ReciprocalOfZero
  => F(ReciprocalOfZero)
```

For normal operations, exceptions escalate:
```Scala
S(x) + S(y) => S(x + y)
E(r) + S(e) => E(r)
S(x) + E(r) => E(r)
```
where `x`, `y` are values, `r` is an exception value, `e` is an expression.

## The `Try` Type
We can use the `Try` type to keep our `repip100` function from throwing an exception and possibly stopping the program. Instead it will return either a `Success` value or a `Failure` corresponding directly to the above `S` and `F`.
```Scala
import scala.util.{Try, Success, Failure}

def recip100(v: Int): Try[Int] =
  if v == 0 then Failure(new ReciprocalOfZero)
  else Success(100 / v)
```

Behind the scenes, `Success(v)` is just a iterable storing a value `v`.
<br>

# Representing Control Flow

> [!cite] someone, probably
> Who the fuck put computer architecture in my Scala?

Slides 15 - 25 show examples of how control flow can be represented, by using an explicit program counter and stack:
![[scala-week11.pdf#page=15]]
<br>

# Caching and more Laziness
**Reminder:** Parameterless function values get reevaluated on every call.
```Scala
> val x = () => {println("Evaluating..."); 42}
  -> val x: () => Int = Lambda/0x000076ed9c5183f0@3e2c8ef

> x()
  -> Evaluating...
     42

> x()
  -> Evaluating...
     42
```

**Reminder:** Lazy values get evaluated only once, from then on they are cached.
```Scala
> lazy val y = {println("Evaluating..."); 42}
  -> lazy val y: Int

> y
  -> Evaluating...
     42

> y
  -> 42
```

## Lazy Fields
We can analyze the above, by defining a class `LazyCell` which has a *lazy field*:
```Scala
class LazyCell[+A](init: => A):
  lazy val get = init
```
Notice, that `init` needs to be defined *by-name*.

Then, we can use it as follows:
```Scala
> val lc = LazyCell({println("Evaluating..."); 42})
  -> val lc: LazyCell[Int] = LazyCell@19b0a9f2

> lc.get
  -> Evaluating...
     42

> lc.get
   -> 42
```
where we observe the same behavior as before, with `lc.get` only being evaluated once.

## Laziness using Mutation
By using a *private `Option` type variable*, we can define a `LazyCell` class which doesn't actually use any lazy values, but behaves the same (not regarding performance probably):
```Scala
class LazyCell[+A](val init: () => A):
  private var cache: Option[A] = None

  def get: A =
    cache match
      case Some(a) => a
      case None => {
        cache = Some(init())
        cache.get
      }
```
Note: By using `private`, we can be sure that the only assignment to `cache` happens inside `get`.

### Proof of Correctness
Slides 34 - 38 outline an invariant proof for the correctness of `LazyCell` implemented as above:
![[scala-week11.pdf#page=34]]
<br>

## Memoization: Caching Function Calls
First of all, let's cache functions without assuming that they will be recursive:
```Scala
case class CachedFunction[-A,+B](val f: A => B): // not just () => B
  private var cache: Map[A,B] = Map()

  def apply(a: A): B =
    cache.get(a) match
      case Some(b) => b
      case None => {
        val b = f(a)
        cache.update(a,b)
        b
      }
```

The above *increases performance* for non-recursive functions, but recursive functions will take exactly the same time, as the cache isn't shared.

### Recursive Functions
Let's take as an example, the Fibonacci function:
```Scala
def fib(n: Int): Int =
  if n == 0 then 0
  else if n == 1 then 1
  else fib(n - 1) + fib(n - 2)
```

We can memoize it by defining the following functions:
```Scala
def fibR(rec: Int => Int, n: Int): Int =
  if n == 0 then 0
  else if n == 1 then 1
  else rec(n - 1) + rec(n - 2)

def memo(H: (Int => Int, Int) => Int): Int => Int = {
  val cache: Map[Int, Int] = Map()

  def rec(a: Int): Int =
    cache.get(a) match
      case Some(b) => b
      case None =>
        val b = H(rec, a) // recursive call to rec
        cache.update(a, b)
        b
  rec
}

def fib(x: Int) = memo(fibR)(x)
```
where the final `fib` definition also caches the intermediate values, and is thus *linear time*.

---

The function `memo` can of course be generic:
```Scala
def memo[A, B](H: (A => B, A) => B): A => B = {
  val cache: Map[A, B] = Map()
  
  def rec(a: A): B =
    cache.get(a) match
      case Some(b) => b
      case None =>
        val b = H(rec,a)
        cache.update(a, b)
        b
  rec
}
```

This function goes from larger values to smaller ones, then uses the map (`cache`) to prevent descending down the same paths again.

The performance can be improved by *first* solving the smaller sub-problems, *then* moving to the larger ones.
- this doesn't improve theoretical complexity
- we usually need to specify it for a given recursive function
- we can avoid checking the map, because we know the result will be in there.

When using integers, instead of a map, we could use an array, where the key is simply the index in the array. This would increase performance even further.

This is part of *dynamic programming*.

